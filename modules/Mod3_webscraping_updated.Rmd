---
title: "Web Scraping"
subtitle: "Lecture Notes"
output:
    html_document: 
        theme: cerulean
---

## Introduction

Many large web sites host a huge amount of information. This information is encoded and delivered on demand to the user within a web page, which is really just a markup language that a browser can understand. We can take this data and analyze it using R, via a variety of different means. Today we'll cover scraping web tables and interacting via Automated Programming Interfaces.

## Ethics and Responsiblity

Many of the tools we'll cover can be quite powerful ways to interact with data stored online and gather it for analysis. Because they're powerful, you need to be careful with them. In particular, try to request information in a way that will not burden the website owners. What constitutes a burden depends on the website. Google, Twitter, Facebook, all of the big websites have protections in place to guard against too many requests and have a huge amount of capacity for taking the requests they grant. Smaller websites may not have either. Always strive to be minimally intrusive: you're usually getting this data for free. 

## Ways of getting data from the web

We will cover several different ways you can get data from the web

1. Directly downloading web pages via the `url()` command. 
2. Scraping simple web tables via `read_html()` and `html_table()` command
3. Interacting with Application Programming Interfaces (APIs) via R libraries that have been designed as "wrappers" for these interfaces, like the awesome `acs` library and the `tigris` library for geographic shapes. 
4. Interacting with APIs directly, 


## Libraries

We will use multiple new libraries today. Among the ones you'll need: 

* `rvest` for scraping websites

* `tidycensus` for accessing American Community Survey data via the census API

* `lubridate` for some date functions

* `gridExtra` to combine graphs

```{r}
rm(list=ls()) #this clears the global environment - if you don't want to clear your GE, then comment this code out (or delete it altogether)

library(tidyverse)
library(rvest)
library(tigris)
library(lubridate)
library(gridExtra)
library(tidycensus)
```

# Scraping web tables -- We aren't going to review this in class, but I'm leaving the code in here in case it's helpful as you begin to look for data on the web. This is just ONE way you can use R to webscrape (i.e., get data from websites that don't have data in an easily downloaded format). There are others (and I have other examples if you need them). 

When we talk about "scraping" a web table, we're talking about pulling a table that exists on a website and turning it into a usable data frame for analysis. Below, I take the table from  `http://en.wikipedia.org/wiki/Marathon_world_record_progression` for men's marathon times and plot the change in speed in m/s as a function of the date that the world record was set. 

```{r}
marathon_wiki <- "https://en.wikipedia.org/wiki/Marathon_world_record_progression"

# Get the page, pull the tables via html_table
marathon <- read_html(marathon_wiki)%>%
  html_table(fill=TRUE)

#Men's is the first table
marathon<-tibble::as_tibble(data.frame(marathon[[1]]))

#Convert marathon time to seconds (our goal is to calculate meters/second)
marathon<-marathon%>%
  mutate(Time2=hms(as.character(Time)))%>%
  mutate(Time2=period_to_seconds(Time2))

#Marathons are 42,200 meters long
marathon$speed<-(42200)/marathon$Time2

#Get dates in a usable format using lubridate::mdy
marathon$date<-mdy(marathon$Date)
```

## Progression of World Record Marathon Speed in Meters/Second
```{r}
marathon<-marathon%>%
  mutate(Nationality=fct_reorder(.f=as_factor(Nationality),
                                 .x=-speed,
                                 .fun = max))
g1<-ggplot(data=marathon,
           aes(y=speed,x=date,
               color=Nationality)
           )  

g1<-g1+geom_point()+
           xlab("Date")+
           ylab("Meters/Second")

g1
```

_Quick Exercise_ Repeat the above analysis for women's world record progression.
```{r}
# Get the page, pull the tables via html_table
marathonwomen <- read_html(marathon_wiki)%>%
  html_table(fill=TRUE)

#Women's is the second table
marathonwomen<-tibble::as_tibble(data.frame(marathonwomen[[2]]))

#Convert marathon time to seconds (our goal is to calculate meters/second)
marathonwomen<-marathonwomen%>%
  mutate(Time2=hms(as.character(Time)))%>%
  mutate(Time3=period_to_seconds(Time2))

#Marathons are 42,200 meters long
marathonwomen$speed<-(42200)/marathonwomen$Time2

#Get dates in a usable format usin lubridate::mdy
marathonwomen$date<-mdy(marathonwomen$Date)

#find median of women's marathon speed
median(marathonwomen$speed, na.rm=T)

#I don't like that it's dropping Marie-Louise's time so I'm going to clean this up and recalculate
marathonwomen$Time[which(marathonwomen$Name=="Marie-Louise Ledru")]<-gsub("xx","00",marathonwomen$Time)
#the line of code above replaces the "xxs" in her time with 00

#Now I will recalculate Time2
marathonwomen<-marathonwomen%>%
  mutate(Time2=hms(as.character(Time)))%>%
  mutate(Time2=period_to_seconds(Time2))

#recalc speed
marathonwomen$speed<-(42200)/marathonwomen$Time2

#find updated median including Marie-Louise
median(marathonwomen$speed, na.rm=T)
```



## API keys

You should have already obtained and installed your Census API key. If not, you can obtain one here:
* The Census API, available here: https://api.census.gov/data/key_signup.html 

PLEASE follow the instructions from the R bootcamp file (stored on the course website) if you haven't installed your key yet. DO NOT INSTALL IT AGAIN when you watch the video with Dr. Doyle. 

# Interacting via APIs

Many websites have created Application Programming Interfaces, which allow the user to directly communicate with the website's underlying database without dealing with the intermediary web content. These have been expanding rapidly and are one of the most exciting areas of development in data access for data science. 

Today, we'll be working with the American Community Survey from the census. Please go to: `http://www.census.gov/developers/` and click on "Request a Key" to get your census key. 

*YOU NEED TO PAY ATTENTION TO TERMS OF USE WHEN USING APIS. DO NOT VIOLATE THESE.*

Next, we'll turn to the American Community Survey (ACS). This includes a large number of tables (available here in excel file form:  https://www.census.gov/programs-surveys/acs/technical-documentation/summary-file-documentation.html) that cover many demographic and other characteristics of the population, down to the level of zip codes. We'll use the `tidycensus` package to get two tables for the counties we're interested in: levels of education and income. We'll turn these tables into two variables: the proportion of the population with incomes above $75,000, and the proportion of the population with at least a bachelor's degree. 

The first step is to get the table from ACS. You can look up these tables by using the `load_variables` command, then searching through the results using`View()` and filter. 
```{r}
## Looking for variables
v18 <- load_variables(2018, "acs5", cache = TRUE)

## put everything in lower case for ease of use
v18<-v18%>%
  mutate(name=str_to_lower(name))%>%
  mutate(label=str_to_lower(label))%>%
  mutate(concept=str_to_lower(concept))
#This is a data frame that includes information on available tables from ACS

#View(v18)

# b15003: education of pop over 25
# b19001: household income over last 12 months
```

You can also use search functions like `str_detect` to find what you're looking for
```{r}
v18%>%
  filter(str_detect(concept,"attainment"))%>%
  filter(str_detect(label,"bachelor"))
```


## Organizing ACS data

The trick with ACS data is organizing it in a way that's going to make sense. For us to get the proportion of individuals with a college degree or more, we're going to need to take the numbers of people who are in each of the various age levels for education, and then divide by the total number of people in the zip code. Below I include code to calculate the proportion of individuals in each zip code who have at least a bachelor's degree. 

Below, I submit a request using my key to get table B15003, which contains information on education levels.
```{r}
## Educ Characteristics by County for Texas
educ_vars<-get_acs(geography="county", state="TX",
                    table="B15003", geometry = TRUE, cache_table = T)

#save(educ_vars,file="educ_vars.Rdata")
## IF THIS DIDN'T WORK FOR SOME REASON YOU CAN LOAD THE FILE (saved to course website)
#load("educ_vars.Rdata")

## Spread, so that each level of education gets its own column
educ_vars<-educ_vars%>%
  select(GEOID, NAME, variable, estimate)%>%
  spread(key=variable, value = estimate)

## rename to be all lower case (for easier coding later)
names(educ_vars)<-str_to_lower(names(educ_vars))

## Calculate prop with at least bachelor's for every county
educ_vars<-educ_vars%>%
  mutate(prop_bach=((b15003_022+b15003_023+b15003_024+b15003_025)/b15003_001)*100)

## simplify to just proportion
educ_vars<-educ_vars%>%
  select(geoid, name, prop_bach, geometry)
```

```{r}
## Income by County for Texas
income_vars<-get_acs(geography = "county", state="TX",
                    table="B19001",
                    geometry=TRUE, cache_table = T)

#save(income_vars,file="income_vars.Rdata")
## IF THIS DIDN'T WORK FOR SOME REASON YOU CAN LOAD THE FILE FROM THE COURSE WEBSITE
#load("income_vars.Rdata")

## Spread, so that each level of education gets its own column
income_vars<-income_vars%>%
  select(GEOID, NAME, variable, estimate)%>%
  spread(key=variable, value = estimate)

## rename to be all lower case 
names(income_vars)<-str_to_lower(names(income_vars))

## Calculate prop with at least $75k for every county
income_vars<-income_vars%>%
  mutate(prop_75=((b19001_013+
                    b19001_014+
                    b19001_015+
                    b19001_016+
                    b19001_017)/b19001_001) *100)                      

## simplify to just proportion
income_vars<-income_vars%>%
  select(geoid, name, prop_75, geometry)
```

```{r}
educ_vars_2<-educ_vars%>%
  as_tibble()%>%
  select(geoid, name, prop_bach)

income_vars_2<-income_vars%>%
  as_tibble()%>%
  select(geoid, name, prop_75)

educ_income<-left_join(educ_vars_2, income_vars_2, by=c("geoid","name"))
```

```{r}
gg<-ggplot(educ_income,aes(y=prop_75p, x=prop_bach)) +
  geom_point() +
  xlab("Proportion of Pop with a Bachelor's") + ylab("Proportion of Pop with Income over 75k")
gg
```

```{r}
gg1<-ggplot(educ_vars, aes(fill=prop_bach)) + 
  geom_sf(color=NA) + 
  scale_fill_viridis_c(option = "viridis") +
  ggtitle("Proportion of Pop with a BA")
gg1
```

```{r}
gg2<-ggplot(income_vars, aes(fill=prop_75p)) +
  geom_sf(color=NA) + 
  scale_fill_viridis_c(option = "viridis") +
  ggtitle("Proportion of Pop with Income over 75k")
gg2
```

```{r}
gg_both<-grid.arrange(gg1, gg2)
gg_both

#another option - if you want them side by side
gg_both2<-ggpubr::ggarrange(gg1, gg2, ncol=2)
gg_both2
```

This resource is amazingly helpful. It means that with a list of zip codes you can get a huge amount of information about the area where the individual resides, including education, housing, income, medical care and other topics. 
```{r}
REMOVE ME
```
---
output:
  pdf_document: default
  html_document: default
---

```{r}
### Install the needed packages...
# install.packages("twitteR")
#install.packages("ROAuth")
# install.packages("rtweet")
#library(arules)
#library(rtweet)
knitr::opts_chunk$set(echo = TRUE)
library(twitteR)
library(ROAuth)
library(jsonlite)
library(rjson)
library(tokenizers)
library(tidyverse)
library(tm)
library(wordcloud)
```

####### Twitter in R

```{r}



## From Gates
#  Consumer API keys
#  Access token & access token secret

## Note is it common to create a text file that contains the
## consumerKey, the comsumerSecret, the access_Token, and the access_Secret
## for security purposes. Instead, for simplicity, I include my secret keys
## below. PLEASE create your own account and use your own keys.
#Insert your consumerKey and consumerSecret below

## Go here for more info:  https://developer.twitter.com/en/account/get-started



consumerKey='SiMslBfTdWEimvLweRDTTrZVH'
consumerSecret='FoPYqK3uwpzutwE6G1RmQvPbRJ8RChFSLfIlgRAcFHjymKDzHh'
access_Token='1084502204038479872-v2czQaDlMt9ikoLnxhiQYk8Yb3f0RT'
access_Secret='U9ktzvd5rEwcK13mttsgwAujS0VxNPtJstxXcEE5znnid'

requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'


#filename="TwitterConKey_ConSec_AccTok_AccSec.txt"
#(tokens<-read.csv(filename, header=TRUE, sep=","))
#(consumerKey=as.character(tokens$consumerKey))
#consumerSecret=as.character(tokens$consumerSecret)
#access_Token=as.character(tokens$access_Token)
#access_Secret=as.character(tokens$access_Secret)
```


NOTES: rtweet is another excellent option
https://mkearney.github.io/blog/2017/06/01/intro-to-rtweet/
 https://rtweet.info/



#  Using twittR 

```{r}
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)

# Below is the function that scours twitter for a particular hash tag.
# n is the number of tweets to be collected

Search<-twitteR::searchTwitter("mardi gras", n=300,since="2020-01-30")
Search_DF <- twListToDF(Search)

# If you wish to store the tweets in a csv file ... 
TransactionTweetsFile = "tweets.csv"
head(Search_DF$text[1])

## You may find that there are many "stopwords" to remove!!
## These are useless words that may confound analyses.

myStopWordList = c("https", "t.co")

## Start the file
Trans <- file(TransactionTweetsFile)
## Tokenize tweets into a list of words 
Tokens<-tokenizers::tokenize_words(Search_DF$text[1],stopwords = append(stopwords::stopwords("en"),myStopWordList), 
          lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,simplify = TRUE)
## Write squished tokens
Tokens["https"]<-""
cat(unlist(str_squish(Tokens)), "\n", file=Trans, sep=",")
close(Trans)

## Append remaining lists of tokens into file
## NOTE - a list of tokens is the set of words from a Tweet
Trans <- file(TransactionTweetsFile, open = "a")
tokenList = Tokens
for(i in 2:nrow(Search_DF)){
  Tokens<-tokenize_words(Search_DF$text[i],stopwords = append(stopwords::stopwords("en"),myStopWordList),  
            lowercase = TRUE,  strip_punct = TRUE, simplify = TRUE)
  Tokens["https"]<-""
  cat(unlist(str_squish(Tokens)), "\n", file=Trans, sep=",")
  tokenList <- c(tokenList,  unlist(str_squish(Tokens)))
}
close(Trans)
```


```{r}

# Create a wordcloud, but first transform list of words into a 
# TermDocumentMatrix

cor <- Corpus(VectorSource(tokenList))

tdm <- TermDocumentMatrix(cor)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

## NOTE:  d contains the words d$word AND frequencies d$freq

wordcloud(d$word,d$freq, colors=c("red","green","blue","orange","black","purple", "seagreen") , random.color = TRUE, min.freq = 10)

```

